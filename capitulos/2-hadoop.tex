\chapter{Hadoop}
\label{cap:hadoop}

Este capítulo irá discutir sobre o software Hadoop.

\section{Hadoop Distributed File System}

Quando uma base de dados atinge a capacidade máxima de espaço provida por uma única máquina física, torna-se necessário distribuir esta responsabilidade com um determinado número de computadores. Sistemas que gerenciam o armazenamento de arquivos em uma ou mais máquinas interligadas em rede são denominados sistemas de arquivos distribuídos. Como seu funcionamento depende dos protocolos de rede que serão utilizados, todos os problemas relacionados a própria rede tornam-se inerentes a este tipo de abordagem, adicionando uma complexidade muito maior aos sistemas de arquivos distribuídos, como por exemplo, em relação aos sistemas de arquivos convencionais.

No contexto de Big Data, que engloba um volume muito grande de dados, a utilização de um mecanismo para armazenar informações ao longo de várias máquinas é indispensável. Neste sentido a camada mais baixa do Hadoop é composta por um sistema de arquivos distribuídos chamado Hadoop Distributed File System (HDFS). Apesar de apresentar semelhanças com os sistemas de arquivos distribuídos existentes seu grande diferencial está na alta capacidade de tolerância a falhas, no baixo custo de hardware que é requerido e também na alta escalabilidade.

\subsection{Principais Características}

O HDFS foi projetado para armazenar arquivos muito grandes a uma taxa de transmissão de dados constante, sendo executado em clusters com hardware de baixo custo2. Suas principais características de design podem ser descritas a seguir:

\begin{enumerate}

\item Arquivos muito grandes: Este sistema de arquivos distribuídos é capaz de armazenar arquivos  que chegam a ocupar terabytes de espaço, portanto é utilizado por aplicações que necessitam de uma base de dados extremamente volumosa. Segundo SHAVACHCKO3, Os clusters do Hadoop utilizados pelo Yahoo chegam a armazenar 25 petabytes de dados.
\item Streaming data access: Operações de leitura podem ser realizadas nos arquivos quantas vezes forem necessárias, porém um arquivo pode ser escrito apenas uma única vez. Esta abordagem pode ser descrita como “write-once, read-many-times”2, este sistema de arquivos distribuídos foi construído partindo da ideia que este modelo é o mais eficiente para processar os dados. O HDFS também realiza a leitura de qualquer arquivo a uma taxa constante, ou seja, a prioridade é garantir vazão da leitura do arquivo e não minimizar a latência ou prover interatividade com aplicações em tempo real, como é realizado em sistemas de arquivos com propósito geral.
\item Hardware de baixo custo: O HDFS foi projetado para ser executado em hardwares que não precisam ter necessariamente alta capacidade computacional e também alta confiabilidade. O sistema foi desenvolvido partindo do pressuposto que a chance dos nós ao longo do cluster falharem é extremamente alta, portanto mecanismos de tolerância a falha foram desenvolvidos para contornar este problema, permitindo uma maior flexibilidade quanto ao uso de diferentes tipos de hardwares.

\end{enumerate}


\subsection{Divisão em Blocos}

Um disco é dividido em vários blocos com tamanho fixo, os quais compõe a menor estrutura de dados, onde são realizadas escritas e leituras. Em sistemas de arquivos convencionais cada bloco não ocupa mais que alguns KB de espaço, geralmente 512 bytes, esta informação é totalmente transparente para o usuário do sistema, no qual apenas realiza operações sobre arquivos de diferentes tamanhos, nele contido.

O HDFS também possui o conceito de blocos, porém estes possuem tamanho bem maior, por padrão ocupam 64MB de espaço. Cada arquivo criado no sistema é quebrado em blocos com estas características, no qual cada um é salvo como uma unidade independente, podendo estar localizado em qualquer nó do cluster. O HDFS foi projetado para armazenar arquivos que ocupam grande quantidade de espaço em disco, portanto com blocos de tamanho elevado o tempo de busca no disco é reduzido significativamente.


\begin{figure}[ht!]
	\centering
	\includegraphics[keepaspectratio=true,scale=0.45]
	  {figuras/hdfs-blocos.eps}
	\caption{Divisão de Arquivos em Blocos}
	\label{fig-hdfs-blocos}
\end{figure}

A utilização de blocos permite simplificar o processo de gerenciamento do armazenamento dos dados. Uma vez que possuem tamanho fixo a tarefa de calcular a quantidade de blocos necessária para todo disco torna-se uma tarefa mais simples. Esta abordagem também permite que blocos sejam replicados pelo sistema de arquivos, provendo tolerância a falhas e uma maior disponibilidade dos dados.

\subsection{Arquitetura}

Um cluster HDFS possui uma arquitetura baseada no modelo mestre/escravo, no qual é composto por um único nó mestre chamado namenode, e um ou mais nós escravos definidos como datanodes. O sistema foi construído utilizando a linguagem de programação java e projetado para ser executado em distribuições GNU/Linux. Portanto para que uma máquina seja um namenode ou datanode é necessário apenas que possua uma JVM disponível juntamente com o sistema operacional adequado.

O namenode é um servidor responsável por gerenciar o namespace do HDFS e também controlar o acesso de clientes aos arquivos contidos no sistema. Este componente mantém a árvore de diretórios e todos os metadados relacionados a ela. Como descrito anteriormente, o HDFS quebra um arquivo em vários blocos e os espalha pelos diversos datanodes do cluster, o namenode possui a tarefa de manter a localização de cada um desses blocos. Todas estas informações são armazenadas no disco local do servidor em dois arquivos: a imagem do sistema de arquivos e o registro de log (WHITE, 2012). O arquivo de imagem do sistema de arquivos também é mantido em memória e é constantemente atualizado.

\begin{figure}[ht!]
	\centering
	\includegraphics[keepaspectratio=true,scale=0.45]
	  {figuras/hdfs-arquitetura.eps}
	\caption{Arquitetura HDFS}
	\label{fig-hdfs-arquitetura}
\end{figure}

Os datanodes representam os nós escravos do HDFS. Eles são responsáveis por armazenar  fisicamente os blocos de arquivos e recuperá-los quando solicitado pelo namenode. Cada datanode se comunica com o namenode do cluster através da camada de transporte TCP/IP, na qual é utilizada uma abstração do protocolo de chamada remota de procedimento (RPC). Periodicamente os datanodes informam ao namenode quais blocos cada um deles está armazenando.

\section{MapReduce}

MapReduce pode ser definido como um paradigma de programação voltado para processamento em \textit{batch} de grande volume de dados ao longo de várias máquinas, obtendo resultados em tempo razoável \cite{white2012}. Utiliza-se o conceito de programação distribuída para resolver problemas, adotando a estratégia de dividi-los em problemas menores e independentes.

De acordo com \citeonline{ghemawatMapreduce2008}, o usuário deste modelo especifica uma função \textit{map}, que deverá processar um par \textit{\{chave, valor\}} gerando como produto conjuntos  intermediários de pares \textit{\{chave, valor\}}, e também define uma função \textit{reduce}, responsável por unir todos os valores intermediários associados a uma mesma chave.

Este modelo é baseado nas primitivas \textit{map} e \textit{reduce} presentes na linguagem \textit{Lisp} e também em muitas outras linguagens de programação funcional. Este paradigma foi adotado pois percebeu-se que vários problemas consistiam em realizar o agrupamento das entradas de acordo com uma chave identificadora, para então processar cada um destes conjuntos \cite{ghemawatMapreduce2008}.

Um programa MapReduce separa arquivos de entrada em diversas partes independentes que servem de entrada para as funções \textit{map}. As saídas destas funções são ordenadas por \textit{\{chave, valor\}} e posteriormente transformadas em entradas do tipo \textit{\{chave, lista(valores)\}} para a função \textit{reduce}, onde o resultado final será salvo em um arquivo de saída. A figura \ref{fig-mapreduce-io} apresenta de maneira genérica as entradas e saídas das funções \textit{map} e \textit{reduce}.

\begin{figure}[ht!]
	\centering
	\includegraphics[keepaspectratio=true,scale=0.7]
	  {figuras/mapreduce-io.eps}
	\caption{Entradas e saídas das funções map e reduce, retirado de \cite{ghemawatMapreduce2008}}
	\label{fig-mapreduce-io}
\end{figure}

A grande contribuição desta abordagem está em disponibilizar uma interface simples e poderosa que permite paralelizar e distribuir computação em larga escala de forma automática \cite{ghemawatMapreduce2008}. O desenvolvedor precisa apenas se preocupar com as funções \textit{map} e \textit{reduce}, todo esforço para dividir o trabalho computacional ao longo das máquinas, entre outras questões operacionais, são de responsabilidade do próprio \textit{framework}.

A figura \ref{fig-mapreduce} ilustra um fluxo simplificado da execução de um programa MapReduce. As entradas são divididas em partes iguais denominadas \textit{input splits}, cada uma destas partes dão origem a uma \textit{map task}, responsável por gerar pares intermediários \textit{\{chave, valor\}}. Cada \textit{map task} realizada uma chamada a função \textit{map} definida pelo usuário.

\begin{figure}[ht!]
	\centering
	\includegraphics[keepaspectratio=true,scale=0.5]
	  {figuras/mapreduce.eps}
	\caption{Fluxo de um programa MapReduce}
	\label{fig-mapreduce}
\end{figure}

Nas fases \textit{shuffle} e \textit{sort} os pares intermediários são agrupados e ordenados de acordo com sua chave. Este processo é realizado pelo \textit{framework} e será detalhado nas seções seguintes, sendo uma das etapas mais complexas de todo fluxo. Por fim, as \textit{reduce tasks} recebem como entrada os valores resultantes das etapas \textit{shuffle} e \textit{sort}. Para cada entrada \textit{\{chave, lista(valores)\}} existente, uma \textit{reduce task} executa uma chamada a função \textit{reduce} especificada pelo desenvolvedor.

\subsection{Contador de Palavras}

Um exemplo simples da aplicabilidade do MapReduce pode ser observado em um problema definido como contador de palavras. Suponha que exista um arquivo de texto com várias palavras inseridas, onde o objetivo seja contar a quantidade de ocorrências de cada uma destas palavras ao longo de todo texto. A princípio parece ser uma atividade trivial, entretanto se o tamanho do arquivo estiver na ordem de gigabytes e aumentarmos a quantidade de arquivos a serem processados o tempo de execução aumentará consideravelmente, tornando-se inviável realizar esta análise.

Uma alternativa para contornar este problema seria o uso da programação paralela, analisando os arquivos em diferentes processos, utilizando quantas \textit{threads} fossem necessárias. Todavia esta solução não é a mais eficiente, já que os arquivos podem apresentar diferentes tamanhos, ou seja, alguns processos seriam finalizados em um intervalo de tempo menor, impossibilitando maximizar a capacidade de processamento. 

Uma abordagem mais eficiente seria separar todos os arquivos em blocos pré definidos e então dividi-los em processos distintos. Esta solução requer um mecanismo de sincronização complexo e de difícil implementação. Seria necessário agrupar todas as palavras e suas respectivas ocorrências em cada uma das \textit{threads} nos diferentes processos em execução. E mesmo assim a capacidade de processamento estaria limitada a apenas uma máquina.

Este problema que se mostrou complexo possui uma solução simples e de fácil construção quando utiliza-se a abordagem apresentada pelo paradigma MapReduce. Nesta situação torna-se necessária apenas a definição de uma função \textit{map} para realizar a contagem de cada palavra presente nos arquivos de entrada, e também de uma função \textit{reduce} para agrupar cada uma destas palavras e realizar a contagem final dos registros de ocorrências. Um pseudo código com as funções map e reduce para este problema são apresentadas na figura \ref{fig-mapreduce-google-code}.

\begin{figure}[ht!]
	\centering
	\includegraphics[keepaspectratio=true,scale=0.8]
	  {figuras/mapreduce-google-code.eps}
	\caption{Pseudo código para contador de palavras, retirado de \cite{ghemawatMapreduce2008}}
	\label{fig-mapreduce-google-code}
\end{figure}

Na figura \ref{fig-mapreduce-google-code} o programa MapReduce divide todos os arquivos de entrada em \textit{input splits}, na qual a chave do par \textit{\{chave, valor\}} é composta pelo número do respectivo \textit{input split}, enquanto o valor é o próprio conteúdo de sua parte no texto. Para cada par de entrada uma função \textit{map} será chamada e realizará quebra da linha em palavras. Cada um destes \textit{tokens} é associado ao valor 1, gerando pares de saída no formato \textit{\{palavra, 1\}}.

O framework MapReduce realiza as etapas \textit{shuffle} e \textit{sort} para agrupar e ordenar os resultados das \textit{map tasks}. Em seguida para cada chave existente será realizada uma chamada a uma função \textit{reduce}, na qual é responsável por percorrer uma lista efetuando a soma dos valores encontrados. O resultado obtido é registrado em um arquivo de saída. A figura \ref{fig-mapreduce-word-count} apresenta o fluxo completo da execução de um programa MapReduce para o problema da contagem de palavras.

\begin{figure}[ht!]
	\centering
	\includegraphics[keepaspectratio=true,scale=0.4]
	  {figuras/mapreduce-word-count.eps}
	\caption{Fluxo de atividades para o contador de palavras}
	\label{fig-mapreduce-word-count}
\end{figure}

\subsection{Arquitetura}

O modelo MapReduce foi criado pela Google para que os programas escritos neste estilo funcional fossem automaticamente paralelizados e executados em um \textit{cluster} composto por hardwares de baixo custo \cite{ghemawatMapreduce2008}. As responsabilidades do \textit{framework} consistem em particionar as entradas em \textit{input splits}, gerenciar a execução e comunicação entre os processos do programa ao longo das máquinas e também tratar as falhas que podem ocorrer em cada nó da rede. De acordo com \citeonline{ghemawatMapreduce2008}, com este cenário um desenvolvedor sem nenhuma experiência em computação paralela seria capaz de desenvolver soluções aplicadas para este contexto. 

Este modelo pode ser aplicado em diversas plataformas, mas o foi projetado principalmente para ser executado sobre o sistema de arquivos distribuídos GFS. Na figura \ref{fig-mapreduce-google} é apresentada a arquitetura do MapReduce, através dela é possível visualizar que um \textit{cluster} é composto por dois tipos de nós: um \textit{master} e diversos \textit{workers}.

Inicialmente os arquivos de entrada são divididos em \textit{input splits} de aproximadamente 64MB, assim como o tamanho dos blocos de arquivos no GFS e também no HDFS. Segundo \citeonline{ghemawatMapreduce2008}, as \textit{map tasks} criadas para cada uma destas entradas são espalhadas pelo \textit{cluster} e executadas em paralelo nos nós do tipo \textit{worker}. As \textit{reduce tasks} criadas para computar os valores intermediários também são processadas em \textit{workers}.

\begin{figure}[ht!]
	\centering
	\includegraphics[keepaspectratio=true,scale=0.5]
	  {figuras/mapreduce-google.eps}
	\caption{Arquitetura MapReduce – Google, retirado de \cite{ghemawatMapreduce2008}}
	\label{fig-mapreduce-google}
\end{figure}

O \textit{MapReduce job} submetido pelo usuário faz o cálculo de quantas \textit{map tasks} serão necessárias para realizar o processamento dos \textit{input splits}. O nó \textit{master} então possui a tarefa de distribuir as \textit{map tasks} e também as \textit{reduce tasks} para os \textit{workers} disponíveis na rede. De acordo com \citeonline{ghemawatMapreduce2008}, o nó \textit{master} também é responsável por manter o estado de todas as funções \textit{map} e \textit{reduce} que são executadas no \textit{cluster}. Estes dados informam se uma determinada tarefa está inativa, em progresso ou finalizada. 

A máquina \textit{master} também monitora os \textit{workers} através de pings que são realizados periodicamente. Caso um \textit{worker} não responda será declarado como indisponível, todas as tarefas deste nó serão reagendadas para serem executadas por outro \textit{worker}. Esta preocupação em monitorar os nós ao longo do \textit{cluster} ocorre porque segundo \citeonline{ghemawatMapreduce2008}, o MapReduce é uma biblioteca projetada para auxiliar no processamento de dados em larga escala ao longo de milhares de máquinas, portanto precisa prover um mecanismo eficiente para tolerar as falhas que ocorrem em computadores da rede.

A largura de banda da rede pode ser considerada um recurso crítico mediante ao contexto apresentado até o momento, a utilização da rede para transportar dados deve ser otimizada ao máximo para que este fator não prejudique o desempenho de um programa MapReduce. Em virtude disso o MapReduce procura tirar vantagem do fato de que os arquivos de entrada são persistidos no disco local dos \textit{workers}. Portanto o \textit{master} obtém a localização de cada \textit{input split} e procura executar as \textit{map tasks} exatamente nestas máquinas \cite{ghemawatMapreduce2008}. Desta forma a execução da fase de mapeamento é realizada localmente, sem consumir os recursos da rede. Segundo \citeonline{white2012}, este processo pode ser definido como \textit{data locality optimization}.

Por sua vez, as \textit{reduce tasks} não possuem a vantagem de serem executadas localmente, pois sua entrada pode estar relacionada às saídas de um conjunto de diferentes \textit{map tasks}. Portanto os valores intermediários gerados pelas funções \textit{map} devem ser transportados pela rede até a máquina onde a função \textit{reduce} está sendo processada.

O Hadoop MapReduce apresenta uma implementação \textit{open-source} consolidada para o modelo MapReduce, na qual é construída a partir da linguagem de programação Java \cite{hadoopSiteMapReduce}, diferentemente do \textit{framework} original desenvolvido em C++ pela Google. A arquitetura do MapReduce pode ser facilmente associada ao paradigma mestre/escravo. A máquina \textit{master} representa o mestre do sistema, enquanto os \textit{workers} simbolizam os escravos. No contexto proposto pelo Hadoop estes elementos são identificados como \textit{jobtracker} e os \textit{tasktrackers}, respectivamente. A figura \ref{fig-mapreduce-arquitetura} ilustra a arquitetura do Hadoop MapReduce.

\begin{figure}[ht!]
	\centering
	\includegraphics[keepaspectratio=true,scale=0.6]
	  {figuras/mapreduce-arquitetura.eps}
	\caption{Arquitetura de um cluster Hadoop}
	\label{fig-mapreduce-arquitetura}
\end{figure}

Assim como no ambiente proposto pela Google, o Hadoop MapReduce também é executado sobre um sistema de arquivos distribuídos em larga escala, no caso o HDFS. Segundo \citeonline{venner2009} É comum o \textit{jobtracker} e o \textit{namenode} estarem localizados na mesma máquina do \textit{cluster}, especialmente em instalações reduzidas.

\subsection{Shuffle e Sort}

Uma das responsabilidades do \textit{framework} MapReduce é garantir que os pares intermediários \textit{\{chave, valor\}} resultantes das funções \textit{maps} sejam ordenados, agrupados e passados como parâmetro para as funções de redução. Esta fase é classificada como \textit{shuffle} e \textit{sort}. De acordo com \citeonline{white2012} esta é a área do código base do Hadoop que está em contínua evolução, podendo ser classificada como o núcleo do MapReduce. A \ref{fig-shuffle} ilustra os processos de \textit{shuffle} e \textit{sort} que ocorrem entre a execução das funções \textit{map} e \textit{reduce}.

\begin{figure}[ht!]
	\centering
	\includegraphics[keepaspectratio=true,scale=0.6]
	  {figuras/shuffle.eps}
	\caption{Fluxograma das etapas Shuffle e Sort, extraído de \cite{white2012}}
	\label{fig-shuffle}
\end{figure}

Durante a execução de uma função \textit{map} os pares \textit{\{chave, valor\}} resultantes são escritos em um \textit{buffer} na memória na medida em que são gerados. Os registros são divididos em R partições, na qual R representa a quantidade de funções \textit{reduce} que seriam necessárias para processar os resultados. Em seguida as partições são ordenadas de acordo com as chaves e escritas no disco local. Segundo \citeonline{white2012}, Os resultados das \textit{map tasks} são escritos no próprio disco da máquina porque se tratam de arquivos intermediários, não há necessidade de armazená-los no sistema de arquivos distribuídos. Esta etapa de particionamento e ordenação é denominada \textit{shuffle}.

Após o término de uma \textit{map task} ocorre a fase de cópia. Nesta etapa as máquinas onde serão executadas as \textit{reduce tasks} são informadas pelo \textit{master} sobre a localização das partições a elas destinadas. As partições geradas pelas \textit{map tasks} são ordenadas localmente justamente para auxiliar neste procedimento. Ao término da cópia de todas as partições ocorre a fase \textit{sort}, onde os resultados são agrupados por chave, mantendo-se a ordenação pela mesma. Desta forma as entradas para as funções de redução estão prontas para serem computadas.




