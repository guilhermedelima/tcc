\chapter{Considerações Finais}
\label{cap:conclusao}

Neste trabalho foi apresentado um estudo em torno da arquitetura e funcionamento do projeto Hadoop, uma das soluções mais conhecidas entre as alternativas existentes para análise de dados no contexto Big Data. A compreensão do novo paradigma proposto pelo modelo MapReduce é o primeiro passo necessário para a inserção de um engenheiro de software no desenvolvimento de aplicações voltadas para processamento de dados em larga escala. Mesmo em situações onde são utilizadas ferramentas com um nível de abstração maior, como por exemplo, Hive, o conhecimento deste modelo torna-se indispensável, pois grande parte dos projetos construídos no topo do Hadoop utilizam internamente o MapReduce.

Com este trabalho foi possível perceber que os objetivos e necessidades do negócio impactam diretamente no tipo de arquitetura que deve ser escolhida para realizar análise de dados em larga escala. O \textit{framework} MapReduce mostrou-se uma alternativa muito interessante para a construção de aplicações que exigem processamento em \textit{batch} de um grande volume de informações. Porém o grande ponto negativo deste modelo está na limitação em realizar escritas e leituras randômicas em arquivos, ou seja, para situações em que são exigidas interações com o usuário a melhor opção é a escolha de um banco de dados NoSQL, como por exemplo, o HBase.

Em casos onde os requisitos se encaixam com as características propostas por uma ferramenta de \textit{data warehouse}, o software Hive é uma escolha a se considerar, pois através de uma linguagem baseada em SQL é possível elaborar consultas poderosas sem a necessidade de codificar \textit{MapReduce jobs}, pois esse processo é feito internamente.
