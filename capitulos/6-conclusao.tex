\chapter{Considerações Finais}
\label{cap:conclusao}

Neste trabalho foi apresentado um estudo em torno da arquitetura e funcionamento do projeto Hadoop, uma das soluções mais conhecidas entre as alternativas existentes para análise de dados no contexto Big Data. A compreensão do novo paradigma proposto pelo modelo MapReduce é o primeiro passo necessário para a inserção de um engenheiro de software no desenvolvimento de aplicações voltadas para processamento de dados em larga escala. Mesmo em situações onde são utilizadas ferramentas com um nível de abstração maior, como o Hive, o conhecimento deste modelo torna-se indispensável, pois grande parte dos projetos construídos no topo do Hadoop utilizam internamente o MapReduce.

Com este trabalho foi possível perceber que os objetivos e necessidades do negócio impactam diretamente no tipo de arquitetura que deve ser escolhida para realizar análise de dados em larga escala. O \textit{framework} MapReduce mostrou-se uma alternativa muito interessante para a construção de aplicações que exigem processamento em \textit{batch} de um grande volume de informações. Porém o grande ponto negativo deste modelo está na limitação para operações de escritas e leituras randômicas em arquivos, ou seja, para situações em que são exigidas interações com o usuário a melhor opção é a escolha de um banco de dados NoSQL, como o HBase.

Em casos onde os requisitos se encaixam com as características propostas por uma ferramenta de \textit{data warehouse}, o software Hive é uma escolha a se considerar, pois através de uma linguagem baseada em SQL é possível elaborar consultas poderosas sem a necessidade de codificar \textit{MapReduce jobs}, pois esse processo é feito internamente.

\section{Trabalhos Futuros}

Os próximos passos deste trabalho consistem na especificação e implementação da arquitetura proposta no capítulo \ref{cap:proposta}. Para tal foram definidas as seguintes macro atividades que devem ser realizadas durante o TCC2 e o cronograma a ser seguido.

\begin{enumerate}

  \item Complementar pesquisas sobre APIs para redes sociais.
  \item Definição e implementação do método de extração de dados das redes sociais.
  \item Continuar pesquisa de algoritmos para análise de publicações de redes sociais.
  \item Instalação e configuração do cluster Hadoop.
  \item Definir ferramentas e implementar a análise dos dados obtidos pela fase de extração.
  \item Desenvolver aplicação \textit{web} para apresentar aos usuários finais os resultados obtidos.
  \item Documentar pesquisa.

\end{enumerate}


\begin{table}[!ht]
\begin{center}
  \begin{tabular}{|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
	\hline
	Atividades & Jul 2014 & Ago 2014 & Set 2014 & Out 2014 & Nov 2014 & Dez 2014
	\\ \hline
	1 & • &   &   &   &   &   	\\ \hline
	2 & • & • &   &   &   &   	\\ \hline
	3 &   & • & • &   &   &   	\\ \hline
	4 & • & • & • & • &   &   	\\ \hline
	5 &   &   & • & • &   &   	\\ \hline
	6 &   &   &   &   & • & •   	\\ \hline
	7 & • & • & • & • & • & •   	\\ \hline
  \end{tabular}
  \caption{Cronograma}
\label{tab-cron}
\end{center}
\end{table}
\FloatBarrier














